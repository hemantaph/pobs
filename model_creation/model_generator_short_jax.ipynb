{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting multiprocessing start method to 'fork'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from ler.utils import get_param_from_json, save_json, load_json\n",
    "import matplotlib.pyplot as plt\n",
    "import pobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Astrophysical parameter distributions\n",
    "\n",
    "## Lensed events\n",
    "\n",
    "* $P_{astro}(m_{1}, m_{2}, \\alpha, \\delta, \\iota, \\chi_{eff}, d_{L,1}, d_{L,2}, \\Delta T_2|H_L)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ra', 'dec', 'theta_jn', 'a_1', 'a_2', 'tilt_1', 'tilt_2', 'mass_1', 'mass_2', 'effective_luminosity_distance', 'effective_geocent_time', 'optimal_snr_net'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get data for detectable events\n",
    "lensed_param = get_param_from_json('/Users/phurailatpamhemantakumar/phd/mypackages/pobs/data/ler_data/n_lensed_detectable_bbh_po_spin.json')\n",
    "\n",
    "# delete the unnecessary keys\n",
    "list_ = ['zl', 'zs', 'sigma', 'q', 'theta_E', 'phi', 'e1', 'e2', 'gamma1', 'gamma2', 'gamma', 'geocent_time', 'phase', 'psi', 'phi_12', 'phi_jl', 'luminosity_distance', 'mass_1_source', 'mass_2_source', 'x0_image_positions', 'x1_image_positions', 'magnifications', 'time_delays', 'image_type', 'n_images', 'L1', 'H1', 'V1']\n",
    "for key in list_:\n",
    "    del lensed_param[key]\n",
    "lensed_param.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate out image 1, 2\n",
    "# this is wrt to time of arrival\n",
    "lensed_param_1 = {}\n",
    "lensed_param_2 = {}\n",
    "\n",
    "for key, value in lensed_param.items():\n",
    "    if np.shape(np.shape(value))[0]==2:\n",
    "        lensed_param_1[key] = value[:,0]\n",
    "        lensed_param_2[key] = value[:,1]\n",
    "    else:\n",
    "        lensed_param_1[key] = value\n",
    "        lensed_param_2[key] = value\n",
    "\n",
    "# For image 1 and 2 only\n",
    "# only keep snr > 8\n",
    "idx_snr1 = lensed_param_1['optimal_snr_net'] > 8\n",
    "idx_snr2 = lensed_param_2['optimal_snr_net'] > 8\n",
    "idx_snr = idx_snr1 & idx_snr2\n",
    "\n",
    "# with effective spin\n",
    "# Note: chi_eff for image 1 and 2 is the same\n",
    "chi_eff = (lensed_param_1['a_1']*np.cos(lensed_param_1['tilt_1']) + lensed_param_1['a_2']*np.cos(lensed_param_1['tilt_2']))/(lensed_param_1['mass_1'] + lensed_param_1['mass_2'])\n",
    "\n",
    "# log10 for (time/86400) and luminosity distance\n",
    "data_dict = dict(\n",
    "    mass_1 = lensed_param_1['mass_1'][idx_snr],\n",
    "    mass_2 = lensed_param_1['mass_2'][idx_snr],\n",
    "    ra = lensed_param_1['ra'][idx_snr],\n",
    "    sindec = np.cos(np.pi/2. - lensed_param_1['dec'][idx_snr]),\n",
    "    costheta_jn = np.cos(lensed_param_1['theta_jn'][idx_snr]),\n",
    "    chi_eff = chi_eff[idx_snr],\n",
    "    log10_dl_1 = np.log10(lensed_param_1['effective_luminosity_distance'][idx_snr]),\n",
    "    log10_dl_2 = np.log10(lensed_param_2['effective_luminosity_distance'][idx_snr]),\n",
    "    log10_dt_12_days = np.log10(((lensed_param_2['effective_geocent_time'][idx_snr] - lensed_param_1['effective_geocent_time'][idx_snr])/86400.)),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $P_{astro}(m_{1}, m_{2}, \\alpha, \\delta, \\iota, \\chi_{eff}, d_{L,1}, d_{L,2}, \\Delta T_2|H_L)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the module\n",
    "# import importlib\n",
    "# importlib.reload(pobs)\n",
    "\n",
    "test = pobs.ModelGenerator(\n",
    "    model_name='astro_lensed', \n",
    "    data_dict=data_dict,\n",
    "    create_new=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax.scipy as jsp\n",
    "from scipy.stats import gaussian_kde\n",
    "import jax.random as random\n",
    "# Initialize a random key\n",
    "key = random.PRNGKey(seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data, _ = test.feature_scaling(data_dict, save=False)\n",
    "kde = gaussian_kde(scaled_data.T, bw_method='scott')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.9 s, sys: 1.8 s, total: 13.7 s\n",
      "Wall time: 11.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([9.86151735e-05, 9.94777471e-05, 1.93593259e-04, ...,\n",
       "       1.41922795e-04, 2.59179438e-04, 4.70536941e-05])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time kde.pdf(scaled_data.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 183 ms, sys: 182 ms, total: 365 ms\n",
      "Wall time: 59.2 ms\n"
     ]
    }
   ],
   "source": [
    "%time new_data = kde.resample(100000).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kde.resample(2).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data_jax = jnp.array(scaled_data)\n",
    "kde_jax = jsp.stats.gaussian_kde(scaled_data_jax.T, bw_method='scott')\n",
    "# np.array(kde_jax.resample(key, (2,)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[-0.09945226, -0.24831685, -1.2515194 ,  1.8546952 ,  0.30575016,\n",
       "         0.84535325, -0.58305115, -0.47416234, -2.4239442 ],\n",
       "       [-1.4755256 , -1.361129  ,  1.133684  , -1.0994499 ,  0.28378958,\n",
       "         0.2701074 , -1.93137   , -0.556511  ,  1.8181386 ]],      dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key = random.PRNGKey(seed=np.random.randint(0, 1000000))\n",
    "kde_jax.resample(key, (2,)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.26 ms, sys: 9.08 ms, total: 14.3 ms\n",
      "Wall time: 4.73 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array([9.8615201e-05, 9.9477780e-05, 1.9359335e-04, ..., 1.4192285e-04,\n",
       "       2.5917956e-04, 4.7053683e-05], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time kde_jax.pdf(scaled_data_jax.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.69 ms, sys: 7.07 ms, total: 13.8 ms\n",
      "Wall time: 4.43 ms\n"
     ]
    }
   ],
   "source": [
    "%time new_data_jax = kde_jax.resample(key, (100000,)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4.34\n",
      "INFO:jax._src.xla_bridge:Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "INFO:jax._src.xla_bridge:Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: dlopen(libtpu.so, 0x0001): tried: 'libtpu.so' (no such file), '/System/Volumes/Preboot/Cryptexes/OSlibtpu.so' (no such file), '/Users/phurailatpamhemantakumar/anaconda3/envs/ler/bin/../lib/libtpu.so' (no such file), '/usr/lib/libtpu.so' (no such file, not in dyld cache), 'libtpu.so' (no such file), '/usr/local/lib/libtpu.so' (no such file), '/usr/lib/libtpu.so' (no such file, not in dyld cache)\n",
      "WARNING:jax._src.xla_bridge:Platform 'METAL' is experimental and not all JAX functionality may be correctly supported!\n",
      "Metal device set to: Apple M2 Pro\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n",
      "[METAL(id=0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1729627924.023560 3541321 mps_client.cc:510] WARNING: JAX Apple GPU support is experimental and not all JAX functionality is correctly supported!\n",
      "I0000 00:00:1729627924.050763 3541321 service.cc:145] XLA service 0x32463cdc0 initialized for platform METAL (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1729627924.050894 3541321 service.cc:153]   StreamExecutor device (0): Metal, <undefined>\n",
      "I0000 00:00:1729627924.052181 3541321 mps_client.cc:406] Using Simple allocator.\n",
      "I0000 00:00:1729627924.052189 3541321 mps_client.cc:384] XLA backend will use up to 11452858368 bytes on device 0 for SimpleAllocator.\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "print(jax.__version__)\n",
    "print(jax.devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the case when you have already created the model\n",
    "# test2 = pobs.ModelGenerator(\n",
    "#     model_name='astro_lensed', \n",
    "#     data_dict=data_dict,\n",
    "#     path_dict=test.path_dict,\n",
    "#     create_new=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get pdf\n",
    "# random_data_dict = test.random(size=10000)\n",
    "# pdf_data_dict = test.pdf(data_dict=random_data_dict)\n",
    "# print(pdf_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot corner\n",
    "# new_data_dict = test.resample(size=20000)\n",
    "# test.plot(data_dict1=new_data_dict, data_dict2=data_dict)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unlensed events\n",
    "\n",
    "* $P_{astro}(m_{1,1}, m_{2,1}, \\alpha_1, \\delta_1, \\iota_1, \\chi_{eff,1}, d_{L,1}|H_U)$\n",
    "\n",
    "* $P_{astro}(m_{1,2}, m_{2,2}, d_{L,2}, \\iota_2, \\chi_{eff,2}, d_{L,2}, \\Delta T_2|H_U)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data for detectable events\n",
    "unlensed_param = get_param_from_json('../data/ler_data/n_unlensed_detectable_bbh_po_spin.json')\n",
    "unlensed_param.keys()\n",
    "\n",
    "list_ = ['zs', 'phase', 'psi', 'phi_12', 'phi_jl', 'mass_1_source', 'mass_2_source', 'L1', 'H1', 'V1', 'optimal_snr_net']\n",
    "\n",
    "# delete the unnecessary keys\n",
    "for key in list_:\n",
    "    del unlensed_param[key]\n",
    "\n",
    "# chi_eff\n",
    "chi_eff = (unlensed_param['a_1']*np.cos(unlensed_param['tilt_1']) + unlensed_param['a_2']*np.cos(unlensed_param['tilt_2']))/(unlensed_param['mass_1'] + unlensed_param['mass_2'])\n",
    "unlensed_param['chi_eff'] = chi_eff\n",
    "del unlensed_param['a_1'], unlensed_param['a_2'], unlensed_param['tilt_1'], unlensed_param['tilt_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will create two sets of unlensed parameters\n",
    "size = 25000\n",
    "idx = np.random.choice(np.arange(0,2*size), 2*size, replace=False)\n",
    "idx1 = idx[:size]\n",
    "idx2 = idx[size:]\n",
    "\n",
    "# let's make sure that 2nd set comes after the first set in time\n",
    "geocent_time1 = unlensed_param['geocent_time'][idx1]\n",
    "geocent_time2 = unlensed_param['geocent_time'][idx2]\n",
    "mask = geocent_time2 < geocent_time1\n",
    "# swap the values\n",
    "# geocent_time1[mask], geocent_time2[mask] = geocent_time2[mask], geocent_time1[mask]\n",
    "new_dict1 = dict()\n",
    "new_dict2 = dict()\n",
    "for key, value in unlensed_param.items():\n",
    "    new_dict1[key] = value[idx1]\n",
    "    new_dict2[key] = value[idx2]\n",
    "    # swap the values\n",
    "    new_dict1[key][mask], new_dict2[key][mask] = new_dict2[key][mask], new_dict1[key][mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log10 for (time/86400) and luminosity distance\n",
    "data_dict = dict(\n",
    "    mass_1 = new_dict1['mass_1'],\n",
    "    mass_2 = new_dict1['mass_2'],\n",
    "    ra = new_dict1['ra'],\n",
    "    sindec = np.cos(np.pi/2. - new_dict1['dec']),\n",
    "    costheta_jn = np.cos(new_dict1['theta_jn']),\n",
    "    chi_eff = new_dict1['chi_eff'],\n",
    "    log10_dl = np.log10(new_dict1['luminosity_distance']),\n",
    ")\n",
    "\n",
    "test = mg.ModelGenerator(\n",
    "    model_name='astro_unlensed1', \n",
    "    data_dict=data_dict,\n",
    ")\n",
    "\n",
    "# test.create_model()\n",
    "\n",
    "# add to a path_dict to a common dictionary\n",
    "path_ = test.pobs_directory+'/path_dict_all.json'\n",
    "path_dict_all = load_json(path_)\n",
    "path_dict_all['astro_unlensed1'] = test.path_dict\n",
    "save_json(path_, path_dict_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = dict(\n",
    "    mass_1 = new_dict2['mass_1'],\n",
    "    mass_2 = new_dict2['mass_2'],\n",
    "    ra = new_dict2['ra'],\n",
    "    sindec = np.cos(np.pi/2. - new_dict2['dec']),\n",
    "    costheta_jn = np.cos(new_dict2['theta_jn']),\n",
    "    chi_eff = new_dict2['chi_eff'],\n",
    "    log10_dl = np.log10(new_dict2['luminosity_distance']),\n",
    "    log10_dt_days = np.log10((new_dict2['geocent_time'] - new_dict1['geocent_time'])/86400.),\n",
    ")\n",
    "\n",
    "test = mg.ModelGenerator(\n",
    "    model_name='astro_unlensed1', \n",
    "    data_dict=data_dict,\n",
    ")\n",
    "\n",
    "# test.create_model()\n",
    "\n",
    "# add to a path_dict to a common dictionary\n",
    "path_ = test.pobs_directory+'/path_dict_all.json'\n",
    "path_dict_all = load_json(path_)\n",
    "path_dict_all['astro_unlensed2'] = test.path_dict\n",
    "save_json(path_, path_dict_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get pdf\n",
    "# random_data_dict = test.random(size=10000)\n",
    "# pdf_data_dict = test.pdf(data_dict=random_data_dict)\n",
    "\n",
    "# # plot corner\n",
    "# new_data_dict = test.resample(size=20000)\n",
    "# test.plot(data_dict1=new_data_dict, data_dict2=data_dict)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posterior distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "l1_ps_1=pd.read_csv('../data/test_data/2_5816_896868863.052364587-1.dat',delimiter='\\t')\n",
    "l1_ps_2=pd.read_csv('../data/test_data/2_5816_896868759.838516950-2.dat',delimiter='\\t')\n",
    "\n",
    "\n",
    "l2_ps_1=pd.read_csv('../data/test_data/2_5864_623721325.912931442-1.dat',delimiter='\\t')\n",
    "l2_ps_2=pd.read_csv('../data/test_data/2_5864_623706766.881841421-2.dat',delimiter='\\t')\n",
    "\n",
    "\n",
    "ul1_ps=pd.read_csv('../data/test_data/502002000.000000000-4004.dat',delimiter='\\t')\n",
    "ul2_ps=pd.read_csv('../data/test_data/502167000.000000000-4334.dat',delimiter='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 1\n",
    "\n",
    "* $P(m_{1,1}, m_{2,1},\\alpha_1, \\delta_1, \\iota_1, \\chi_{eff,1}, d_{L,1}|T_1,d_1)$ or $P(.|T_1,d_1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l1_ps_1\n",
    "data_dict = dict(\n",
    "    mass_1 = l1_ps_1['m1'],\n",
    "    mass_2 = l1_ps_1['m2'],\n",
    "    ra = l1_ps_1['ra'],\n",
    "    sindec = np.cos(np.pi/2. - l1_ps_1['dec']),\n",
    "    costheta_jn = np.cos(l1_ps_1['theta_jn']),\n",
    "    chi_eff = l1_ps_1['chi_eff'],\n",
    "    log10_dl = np.log10(l1_ps_1['distance']),\n",
    ")\n",
    "\n",
    "test = mg.ModelGenerator(\n",
    "    model_name='posterior_l1_ps_1', \n",
    "    data_dict=data_dict,\n",
    ")\n",
    "\n",
    "# test.create_model()\n",
    "\n",
    "# add to a path_dict to a common dictionary\n",
    "path_ = test.pobs_directory+'/path_dict_all.json'\n",
    "path_dict_all = load_json(path_)\n",
    "path_dict_all['posterior_l1_ps_1'] = test.path_dict\n",
    "save_json(path_, path_dict_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l2_ps_1\n",
    "data_dict = dict(\n",
    "    mass_1 = l2_ps_1['m1'],\n",
    "    mass_2 = l2_ps_1['m2'],\n",
    "    ra = l2_ps_1['ra'],\n",
    "    sindec = np.cos(np.pi/2. - l2_ps_1['dec']),\n",
    "    costheta_jn = np.cos(l2_ps_1['theta_jn']),\n",
    "    chi_eff = l2_ps_1['chi_eff'],\n",
    "    log10_dl = np.log10(l2_ps_1['distance']),\n",
    ") \n",
    "\n",
    "test = mg.ModelGenerator(\n",
    "    model_name='posterior_l2_ps_1', \n",
    "    data_dict=data_dict,\n",
    ")\n",
    "\n",
    "# test.create_model()\n",
    "\n",
    "# add to a path_dict to a common dictionary\n",
    "path_ = test.pobs_directory+'/path_dict_all.json'\n",
    "path_dict_all = load_json(path_)\n",
    "path_dict_all['posterior_l2_ps_1'] = test.path_dict\n",
    "save_json(path_, path_dict_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ul1_ps\n",
    "data_dict = dict(\n",
    "    mass_1 = ul1_ps['m1'],\n",
    "    mass_2 = ul1_ps['m2'],\n",
    "    ra = ul1_ps['ra'],\n",
    "    sindec = np.cos(np.pi/2. - ul1_ps['dec']),\n",
    "    costheta_jn = np.cos(ul1_ps['theta_jn']),\n",
    "    chi_eff = ul1_ps['chi_eff'],\n",
    "    log10_dl = np.log10(ul1_ps['dist']),\n",
    ")\n",
    "\n",
    "test = mg.ModelGenerator(\n",
    "    model_name='posterior_ul1_ps', \n",
    "    data_dict=data_dict,\n",
    ")\n",
    "\n",
    "# test.create_model()\n",
    "\n",
    "# add to a path_dict to a common dictionary\n",
    "path_ = test.pobs_directory+'/path_dict_all.json'\n",
    "path_dict_all = load_json(path_)\n",
    "path_dict_all['posterior_ul1_ps'] = test.path_dict\n",
    "save_json(path_, path_dict_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 2\n",
    "\n",
    "* $P(m_{1,2}, m_{2,2},\\alpha_2, \\delta_2, \\iota_2, \\chi_{eff,2}, d_{L,2}|T_2, d_2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l1_ps_2\n",
    "data_dict = dict(\n",
    "    mass_1 = l1_ps_2['m1'],\n",
    "    mass_2 = l1_ps_2['m2'],\n",
    "    ra = l1_ps_2['ra'],\n",
    "    sindec = np.cos(np.pi/2. - l1_ps_2['dec']),\n",
    "    costheta_jn = np.cos(l1_ps_2['theta_jn']),\n",
    "    chi_eff = l1_ps_2['chi_eff'],\n",
    "    log10_dl = np.log10(l1_ps_2['distance']),\n",
    ")\n",
    "\n",
    "test = mg.ModelGenerator(\n",
    "    model_name='posterior_l1_ps_2', \n",
    "    data_dict=data_dict,\n",
    ")\n",
    "\n",
    "# test.create_model()\n",
    "\n",
    "# add to a path_dict to a common dictionary\n",
    "path_ = test.pobs_directory+'/path_dict_all.json'\n",
    "path_dict_all = load_json(path_)\n",
    "path_dict_all['posterior_l1_ps_2'] = test.path_dict\n",
    "save_json(path_, path_dict_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l2_ps_2\n",
    "data_dict = dict(\n",
    "    mass_1 = l2_ps_2['m1'],\n",
    "    mass_2 = l2_ps_2['m2'],\n",
    "    ra = l2_ps_2['ra'],\n",
    "    sindec = np.cos(np.pi/2. - l2_ps_2['dec']),\n",
    "    costheta_jn = np.cos(l2_ps_2['theta_jn']),\n",
    "    chi_eff = l2_ps_2['chi_eff'],\n",
    "    log10_dl = np.log10(l2_ps_2['distance']),\n",
    ")\n",
    "\n",
    "test = mg.ModelGenerator(\n",
    "    model_name='posterior_l2_ps_2', \n",
    "    data_dict=data_dict,\n",
    ")\n",
    "\n",
    "# test.create_model()\n",
    "\n",
    "# add to a path_dict to a common dictionary\n",
    "path_ = test.pobs_directory+'/path_dict_all.json'\n",
    "path_dict_all = load_json(path_)\n",
    "path_dict_all['posterior_l2_ps_2'] = test.path_dict\n",
    "save_json(path_, path_dict_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ul2_ps\n",
    "data_dict = dict(\n",
    "    mass_1 = ul2_ps['m1'],\n",
    "    mass_2 = ul2_ps['m2'],\n",
    "    ra = ul2_ps['ra'],\n",
    "    sindec = np.cos(np.pi/2. - ul2_ps['dec']),\n",
    "    costheta_jn = np.cos(ul2_ps['theta_jn']),\n",
    "    chi_eff = ul2_ps['chi_eff'],\n",
    "    log10_dl = np.log10(ul2_ps['dist']),\n",
    ")\n",
    "\n",
    "test = mg.ModelGenerator(\n",
    "    model_name='posterior_ul2_ps', \n",
    "    data_dict=data_dict,\n",
    ")\n",
    "\n",
    "# test.create_model()\n",
    "\n",
    "# add to a path_dict to a common dictionary\n",
    "path_ = test.pobs_directory+'/path_dict_all.json'\n",
    "path_dict_all = load_json(path_)\n",
    "path_dict_all['posterior_ul2_ps'] = test.path_dict\n",
    "save_json(path_, path_dict_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Dataset\n",
    "\n",
    "* $P_{comb}(m_{1}, m_{2}, \\alpha, \\delta, \\iota, \\chi_{eff}, d_{L,1}, d_{L,2}|d_1, d_2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l1_ps_1, l1_ps_2\n",
    "# check l1_ps_2 or l1_ps_1 have more data points\n",
    "len_ = len(l1_ps_2['m1'])\n",
    "if len(l1_ps_1['m1']) < len_:\n",
    "    len_ = len(l1_ps_1['m1'])\n",
    "len_ = int(len_/2)-1\n",
    "\n",
    "data_dict = dict(\n",
    "    mass_1 = np.concatenate((l1_ps_1['m1'][0:len_], l1_ps_2['m1'][0:len_])),\n",
    "    mass_2 = np.concatenate((l1_ps_1['m2'][0:len_], l1_ps_2['m2'][0:len_])),\n",
    "    ra = np.concatenate((l1_ps_1['ra'][0:len_], l1_ps_2['ra'][0:len_])),\n",
    "    sindec = np.cos(np.pi/2. - np.concatenate((l1_ps_1['dec'][0:len_], l1_ps_2['dec'][0:len_]))),\n",
    "    costheta_jn = np.cos(np.concatenate((l1_ps_1['theta_jn'][0:len_], l1_ps_2['theta_jn'][0:len_]))),\n",
    "    chi_eff = np.concatenate((l1_ps_1['chi_eff'][0:len_], l1_ps_2['chi_eff'][0:len_])),\n",
    "    log10_dl_1 = np.log10(l1_ps_1['distance'][0:(2*len_)]),\n",
    "    log10_dl_2 = np.log10(l1_ps_2['distance'][0:(2*len_)]),\n",
    ")\n",
    "\n",
    "if not np.median(l1_ps_1['time'][0:2*len_])-np.median(l1_ps_2['time'][0:2*len_])>0:\n",
    "    data_dict['log10_dl_1'], data_dict['log10_dl_2'] = data_dict['log10_dl_2'], data_dict['log10_dl_1']\n",
    "\n",
    "test = mg.ModelGenerator(\n",
    "    model_name='posterior_l1_ps_1_2', \n",
    "    data_dict=data_dict,\n",
    ")\n",
    "\n",
    "# test.create_model()\n",
    "\n",
    "# add to a path_dict to a common dictionary\n",
    "path_ = test.pobs_directory+'/path_dict_all.json'\n",
    "path_dict_all = load_json(path_)\n",
    "path_dict_all['posterior_l1_ps_1_2'] = test.path_dict\n",
    "save_json(path_, path_dict_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l2_ps_1, l2_ps_2\n",
    "# check l2_ps_2 or l2_ps_1 have more data points\n",
    "len_ = len(l2_ps_2['m1'])\n",
    "if len(l2_ps_1['m1']) < len_:\n",
    "    len_ = len(l2_ps_1['m1'])\n",
    "len_ = int(len_/2)-1\n",
    "\n",
    "data_dict = dict(\n",
    "    mass_1 = np.concatenate((l2_ps_1['m1'][0:len_], l2_ps_2['m1'][0:len_])),\n",
    "    mass_2 = np.concatenate((l2_ps_1['m2'][0:len_], l2_ps_2['m2'][0:len_])),\n",
    "    ra = np.concatenate((l2_ps_1['ra'][0:len_], l2_ps_2['ra'][0:len_])),\n",
    "    sindec = np.cos(np.pi/2. - np.concatenate((l2_ps_1['dec'][0:len_], l2_ps_2['dec'][0:len_]))),\n",
    "    costheta_jn = np.cos(np.concatenate((l2_ps_1['theta_jn'][0:len_], l2_ps_2['theta_jn'][0:len_]))),\n",
    "    chi_eff = np.concatenate((l2_ps_1['chi_eff'][0:len_], l2_ps_2['chi_eff'][0:len_])),\n",
    "    log10_dl_1 = np.log10(l2_ps_1['distance'][0:(2*len_)]),\n",
    "    log10_dl_2 = np.log10(l2_ps_2['distance'][0:(2*len_)]),\n",
    ")\n",
    "\n",
    "if not np.median(l2_ps_1['time'][0:2*len_])-np.median(l2_ps_2['time'][0:2*len_])>0:\n",
    "    data_dict['log10_dl_1'], data_dict['log10_dl_2'] = data_dict['log10_dl_2'], data_dict['log10_dl_1']\n",
    "\n",
    "test = mg.ModelGenerator(\n",
    "    model_name='posterior_l2_ps_1_2', \n",
    "    data_dict=data_dict,\n",
    ")\n",
    "\n",
    "# test.create_model()\n",
    "\n",
    "# add to a path_dict to a common dictionary\n",
    "path_ = test.pobs_directory+'/path_dict_all.json'\n",
    "path_dict_all = load_json(path_)\n",
    "path_dict_all['posterior_l2_ps_1_2'] = test.path_dict\n",
    "save_json(path_, path_dict_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ul1_ps, ul2_ps\n",
    "# check ul2_ps or ul1_ps have more data points\n",
    "len_ = len(ul2_ps['m1'])\n",
    "if len(ul1_ps['m1']) < len_:\n",
    "    len_ = len(ul1_ps['m1'])\n",
    "len_ = int(len_/2)-1\n",
    "\n",
    "data_dict = dict(\n",
    "    mass_1 = np.concatenate((ul1_ps['m1'][0:len_], ul2_ps['m1'][0:len_])),\n",
    "    mass_2 = np.concatenate((ul1_ps['m2'][0:len_], ul2_ps['m2'][0:len_])),\n",
    "    ra = np.concatenate((ul1_ps['ra'][0:len_], ul2_ps['ra'][0:len_])),\n",
    "    sindec = np.cos(np.pi/2. - np.concatenate((ul1_ps['dec'][0:len_], ul2_ps['dec'][0:len_]))),\n",
    "    costheta_jn = np.cos(np.concatenate((ul1_ps['theta_jn'][0:len_], ul2_ps['theta_jn'][0:len_]))),\n",
    "    chi_eff = np.concatenate((ul1_ps['chi_eff'][0:len_], ul2_ps['chi_eff'][0:len_])),\n",
    "    log10_dl_1 = np.log10(ul1_ps['dist'][0:(2*len_)]),\n",
    "    log10_dl_2 = np.log10(ul2_ps['dist'][0:(2*len_)]),\n",
    ")\n",
    "\n",
    "if not np.median(ul1_ps['time'][0:2*len_])-np.median(ul2_ps['time'][0:2*len_])>0:\n",
    "    data_dict['log10_dl_1'], data_dict['log10_dl_2'] = data_dict['log10_dl_2'], data_dict['log10_dl_1']\n",
    "\n",
    "test = mg.ModelGenerator(\n",
    "    model_name='posterior_ul1_ps_2', \n",
    "    data_dict=data_dict,\n",
    ")\n",
    "\n",
    "# test.create_model()\n",
    "\n",
    "# add to a path_dict to a common dictionary\n",
    "path_ = test.pobs_directory+'/path_dict_all.json'\n",
    "path_dict_all = load_json(path_)\n",
    "path_dict_all['posterior_ul1_ps_2'] = test.path_dict\n",
    "save_json(path_, path_dict_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mass and effective spin prior\n",
    "\n",
    "* $P(m_{1}, m_{2}, \\chi_{eff})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02:45 bilby INFO    : No prior given, using default BBH priors in /Users/phurailatpamhemantakumar/anaconda3/envs/ler/lib/python3.10/site-packages/bilby/gw/prior_files/precessing_spins_bbh.prior.\n"
     ]
    }
   ],
   "source": [
    "import bilby\n",
    "\n",
    "# sample m1, m2, ra, dec, cos(theta_jn), mc, chi_eff, distance\n",
    "# sample from the bilby prior\n",
    "prior = bilby.gw.prior.BBHPriorDict()\n",
    "mc = prior['chirp_mass'].sample(size)\n",
    "q = prior['mass_ratio'].sample(size)\n",
    "m1 = mc * (1 + q)**(1/5) * q**(3/5)\n",
    "m2 = m1 * q\n",
    "a_1 = prior['a_1'].sample(size) \n",
    "a_2 = prior['a_2'].sample(size)\n",
    "tilt_1 = prior['tilt_1'].sample(size)\n",
    "tilt_2 = prior['tilt_2'].sample(size)\n",
    "chi_eff = (a_1*np.cos(tilt_1) + a_2*np.cos(tilt_2))/(m1 + m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = dict(\n",
    "    mass_1 = m1,\n",
    "    mass_2 = m2,\n",
    "    chi_eff = chi_eff,\n",
    ")\n",
    "\n",
    "test = mg.ModelGenerator(\n",
    "    model_name='prior_m1m2chieff', \n",
    "    data_dict=data_dict,\n",
    ")\n",
    "\n",
    "# test.create_model()\n",
    "\n",
    "# add to a path_dict to a common dictionary\n",
    "path_ = test.pobs_directory+'/path_dict_all.json'\n",
    "path_dict_all = load_json(path_)\n",
    "path_dict_all['prior_m1m2chieff'] = test.path_dict\n",
    "save_json(path_, path_dict_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot corner\n",
    "# new_data_dict = test.resample(size=40000)\n",
    "# test.plot(data_dict1=new_data_dict, data_dict2=data_dict)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['astro_lensed', 'astro_unlensed1', 'astro_unlensed2', 'posterior_l1_ps_1', 'posterior_l2_ps_1', 'posterior_ul1_ps', 'posterior_l1_ps_2', 'posterior_l2_ps_2', 'posterior_ul2_ps', 'posterior_l1_ps_1_2', 'posterior_l2_ps_1_2', 'posterior_ul1_ps_2', 'prior_m1m2chieff'])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_dict_all = load_json(path_)\n",
    "path_dict_all.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_path': './pobs_data/model_path_astro_lensed.pkl',\n",
       " 'scaler_path': './pobs_data/scaler_path_astro_lensed.pkl',\n",
       " 'min_max_path': './pobs_data/min_max_path_astro_lensed.pkl',\n",
       " 'label_list': ['mass_1',\n",
       "  'mass_2',\n",
       "  'ra',\n",
       "  'sindec',\n",
       "  'costheta_jn',\n",
       "  'chi_eff',\n",
       "  'log10_dl_1',\n",
       "  'log10_dl_2',\n",
       "  'log10_dt_12_days'],\n",
       " 'bandwidth_factor': 0.25}"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_dict_all['astro_lensed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ler",
   "language": "python",
   "name": "ler"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
